<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Nishant Kambhatla | publications</title>
<meta name="description" content="Nishant Kambhatla's academic website. PhD student at SFU Computer Science : Natural Language Processing , Deep Learning , Neural Machine Translation .
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/css/mdb.min.css" integrity="sha256-/SwJ2GDcEt5382i8zqDwl36VJGECxEoIcBIuoLmLR4g=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"  integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Open Graph -->


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
        
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Nishant</span> Kambhatla</a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">Publications in reverse chronological order. Powered by Jekyll-Scholar.</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2023</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EACL</abbr>
    
  
  </div>

  <div id="kambhatla-etal-2023-decipherment" class="col-sm-8">
    
      <span class="title">Decipherment as Regression: Solving Historical Substitution Ciphers by Learning Symbol Recurrence Relations</span>
      <span class="author">
        
          
            
              
                <em>Kambhatla, Nishant</em>,
              
            
          
        
          
            
              
                
                  <a href="https://mrlogarithm.github.io/about-me/about.html" target="_blank">Born, Logan</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.cs.sfu.ca/~anoop/" target="_blank">Sarkar, Anoop</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Findings of the Association for Computational Linguistics: EACL 2023</em>
      
      
        2023
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://aclanthology.org/2023.findings-eacl.160" target="_blank">URL</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Solving substitution ciphers involves mapping sequences of cipher symbols to fluent text in a target language. This has conventionally been formulated as a search problem, to find the decipherment key using a character-level language model to constrain the search space. This work instead frames decipherment as a sequence prediction task, using a Transformer-based causal language model to learn recurrences between characters in a ciphertext. We introduce a novel technique for transcribing arbitrary substitution ciphers into a common recurrence encoding. By leveraging this technique, we (i) create a large synthetic dataset of homophonic ciphers using random keys, and (ii) train a decipherment model that predicts the plaintext sequence given a recurrence-encoded ciphertext. Our method achieves strong results on synthetic 1:1 and homophonic ciphers, and cracks several real historic homophonic ciphers. Our analysis shows that the model learns recurrence relations between cipher symbols and recovers decipherment keys in its self-attention.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IWSLT@ACL</abbr>
    
  
  </div>

  <div id="kambhatla-etal-2023-learning" class="col-sm-8">
    
      <span class="title">Learning Nearest Neighbour Informed Latent Word Embeddings to Improve Zero-Shot Machine Translation</span>
      <span class="author">
        
          
            
              
                <em>Kambhatla, Nishant</em>,
              
            
          
        
          
            
              
                
                  <a href="https://mrlogarithm.github.io/about-me/about.html" target="_blank">Born, Logan</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.cs.sfu.ca/~anoop/" target="_blank">Sarkar, Anoop</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)</em>
      
      
        2023
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://aclanthology.org/2023.iwslt-1.27" target="_blank">URL</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Multilingual neural translation models exploit cross-lingual transfer to perform zero-shot translation between unseen language pairs. Past efforts to improve cross-lingual transfer have focused on aligning contextual sentence-level representations. This paper introduces three novel contributions to allow exploiting nearest neighbours at the token level during training, including: (i) an efficient, gradient-friendly way to share representations between neighboring tokens; (ii) an attentional semantic layer which extracts latent features from shared embeddings; and (iii) an agreement loss to harmonize predictions across different sentence representations. Experiments on two multilingual datasets demonstrate consistent gains in zero shot translation over strong baselines.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IWSLT@ACL</abbr>
    
  
  </div>

  <div id="jain-etal-2023-language" class="col-sm-8">
    
      <span class="title">Language Model Based Target Token Importance Rescaling for Simultaneous Neural Machine Translation</span>
      <span class="author">
        
          
            
              
                <em>Kambhatla, Nishant*</em>,
              
            
          
        
          
            
              
                
                  <a href="https://aditijain14.github.io" target="_blank">Jain, Aditi*</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.cs.sfu.ca/~anoop/" target="_blank">Sarkar, Anoop</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)</em>
      
      
        2023
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://aclanthology.org/2023.iwslt-1.32" target="_blank">URL</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>The decoder in simultaneous neural machine translation receives limited information from the source while having to balance the opposing requirements of latency versus translation quality. In this paper, we use an auxiliary target-side language model to augment the training of the decoder model. Under this notion of target adaptive training, generating rare or difficult tokens is rewarded which improves the translation quality while reducing latency. The predictions made by a language model in the decoder are combined with the traditional cross entropy loss which frees up the focus on the source side context. Our experimental results over multiple language pairs show that compared to previous state of the art methods in simultaneous translation, we can use an augmented target side context to improve BLEU scores significantly. We show improvements over the state of the art in the low latency range with lower average lagging values (faster output).</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EAMT</abbr>
    
  
  </div>

  <div id="kambhatla-etal-2022-auxiliary" class="col-sm-8">
    
      <span class="title">Auxiliary Subword Segmentations as Related Languages for Low Resource Multilingual Translation</span>
      <span class="author">
        
          
            
              
                <em>Kambhatla, Nishant</em>,
              
            
          
        
          
            
              
                
                  <a href="https://mrlogarithm.github.io/about-me/about.html" target="_blank">Born, Logan</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.cs.sfu.ca/~anoop/" target="_blank">Sarkar, Anoop</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the The 23rd Annual Conference of the European Association for Machine Translation (To Appear)</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
      [<a href="/assets/pdf/multisub_eamt2022.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We propose a novel technique of combining multiple subword tokenizations of a single source-target language pair for use with multilingual neural translation training methods. These alternate segmentations function like related languages in multilingual translation, improving translation accuracy for low-resource languages and producing translations that are lexically diverse and morphologically rich. We also introduce a cross-teaching technique which yields further improvements in translation accuracy and cross-lingual transfer between high- and low-resource language pairs. Compared to other strong multilingual baselines, our approach yields average gains of +1.7 BLEU across the four low-resource datasets from the multilingual TED-talks dataset. Our technique does not require additional training data and is a drop-in improvement for any existing neural translation system.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="kambhatla-etal-2022-cipherdaug" class="col-sm-8">
    
      <span class="title">CipherDAug: Ciphertext Based Data Augmentation for Neural Machine Translation</span>
      <span class="author">
        
          
            
              
                <em>Kambhatla, Nishant</em>,
              
            
          
        
          
            
              
                
                  <a href="https://mrlogarithm.github.io/about-me/about.html" target="_blank">Born, Logan</a>,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.cs.sfu.ca/~anoop/" target="_blank">Sarkar, Anoop</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Long Paper (To Appear)</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
      [<a href="/assets/pdf/cipherdaug_acl2022.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We propose a novel data-augmentation technique for neural machine translation based on ROT-k ciphertexts. ROT-k is a simple letter substitution cipher that replaces a letter in the plaintext with the kth letter after it in the alphabet. We first generate multiple ROT-k ciphertexts using different values of k for the plaintext which is the source side of the parallel data. We then leverage this enciphered training data along with the original parallel data via multi-source training to improve neural machine translation. Our method, CipherDAug, uses a co-regularization-inspired training procedure, requires no external data sources other than the original training data, and uses a standard Transformer to outperform strong data augmentation techniques on several datasets by a significant margin. This technique combines easily with existing approaches to data augmentation, and yields particularly strong results in low-resource settings.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EACL</abbr>
    
  
  </div>

  <div id="moradi-etal-2021-measuring" class="col-sm-8">
    
      <span class="title">Measuring and Improving Faithfulness of Attention in Neural Machine Translation</span>
      <span class="author">
        
          
            
              
                
                  Moradi, Pooya,
                
              
            
          
        
          
            
              
                <em>Kambhatla, Nishant</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.cs.sfu.ca/~anoop/" target="_blank">Sarkar, Anoop</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.aclweb.org/anthology/2021.eacl-main.243" target="_blank">URL</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>While the attention heatmaps produced by neural machine translation (NMT) models seem insightful, there is little evidence that they reflect a model’s true internal reasoning. We provide a measure of faithfulness for NMT based on a variety of stress tests where attention weights which are crucial for prediction are perturbed and the model should alter its predictions if the learned weights are a faithful explanation of the predictions. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and has a useful regularization effect on the NMT model and can even improve translation quality in some cases.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AACL SRW</abbr>
    
  
  </div>

  <div id="moradi-etal-2020-training" class="col-sm-8">
    
      <span class="title">Training with Adversaries to Improve Faithfulness of Attention in Neural Machine Translation</span>
      <span class="author">
        
          
            
              
                
                  Moradi, Pooya,
                
              
            
          
        
          
            
              
                <em>Kambhatla, Nishant</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.cs.sfu.ca/~anoop/" target="_blank">Sarkar, Anoop</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.aclweb.org/anthology/2020.aacl-srw.14" target="_blank">URL</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Can we trust that the attention heatmaps produced by a neural machine translation (NMT) model reflect its true internal reasoning? We isolate and examine in detail the notion of faithfulness in NMT models. We provide a measure of faithfulness for NMT based on a variety of stress tests where model parameters are perturbed and measuring faithfulness based on how often the model output changes. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and it also seems to have a useful regularization effect on the NMT model and can even improve translation quality in some cases.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL WS</abbr>
    
  
  </div>

  <div id="born-etal-2019-sign" class="col-sm-8">
    
      <span class="title">Sign Clustering and Topic Extraction in Proto-Elamite</span>
      <span class="author">
        
          
            
              
                
                  <a href="https://mrlogarithm.github.io/about-me/about.html" target="_blank">Born, Logan</a>,
                
              
            
          
        
          
            
              
                
                  <a href="https://cnrs.ubc.ca/people/kate-kelley/" target="_blank">Kelley, Kate</a>,
                
              
            
          
        
          
            
              
                <em>Kambhatla, Nishant</em>,
              
            
          
        
          
            
              
                
                  Chen, Carolyn,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.cs.sfu.ca/~anoop/" target="_blank">Sarkar, Anoop</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature @ NAACL</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.aclweb.org/anthology/W19-2516" target="_blank">URL</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We describe a first attempt at using techniques from computational linguistics to analyze the undeciphered proto-Elamite script. Using hierarchical clustering, n-gram frequencies, and LDA topic models, we both replicate results obtained by manual decipherment and reveal previously-unobserved relationships between signs. This demonstrates the utility of these techniques as an aid to manual decipherment.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP WS</abbr>
    
  
  </div>

  <div id="moradi2019interrogating" class="col-sm-8">
    
      <span class="title">Interrogating the Explanatory Power of Attention in Neural Machine Translation</span>
      <span class="author">
        
          
            
              
                
                  Moradi, Pooya,
                
              
            
          
        
          
            
              
                <em>Kambhatla, Nishant</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.cs.sfu.ca/~anoop/" target="_blank">Sarkar, Anoop</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>The 3rd Workshop on Neural Generation and Translation @ EMNLP-IJCNLP</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.aclweb.org/anthology/D19-5624.pdf" target="_blank">URL</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Attention models have become a crucial component in neural machine translation (NMT). They are often implicitly or explicitly used to justify the model’s decision in generating a specific token but it has not yet been rigorously established to what extent attention is a reliable source of information in NMT. To evaluate the explanatory power of attention for NMT, we examine the possibility of yielding the same prediction but with counterfactual attention models that modify crucial aspects of the trained attention model. Using these counterfactual attention mechanisms we assess the extent to which they still preserve the generation of function and content words in the translation process. Compared to a state of the art attention model, our counterfactual attention models produce 68% of function words and 21% of content words in our German-English dataset. Our experiments demonstrate that attention models by themselves cannot reliably explain the decisions made by a NMT model.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="kambhatla-etal-2018-decipherment" class="col-sm-8">
    
      <span class="title">Decipherment of Substitution Ciphers with Neural Language Models</span>
      <span class="author">
        
          
            
              
                <em>Kambhatla, Nishant</em>,
              
            
          
        
          
            
              
                
                  Mansouri Bigvand, Anahita,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.cs.sfu.ca/~anoop/" target="_blank">Sarkar, Anoop</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>
      
      
        2018
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.aclweb.org/anthology/D18-1102" target="_blank">URL</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Decipherment of homophonic substitution ciphers using language models is a well-studied task in NLP. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram language models. The most widely used technique is the use of beam search with n-gram language models proposed by Nuhn et al.(2013). We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural language model. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural language model. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP WS</abbr>
    
  
  </div>

  <div id="wu-etal-2018-decipherment" class="col-sm-8">
    
      <span class="title">Decipherment for Adversarial Offensive Language Detection</span>
      <span class="author">
        
          
            
              
                
                  Wu, Zhelun,
                
              
            
          
        
          
            
              
                <em>Kambhatla, Nishant</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.cs.sfu.ca/~anoop/" target="_blank">Sarkar, Anoop</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2) @ EMNLP</em>
      
      
        2018
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.aclweb.org/anthology/W18-5119" target="_blank">URL</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Automated filters are commonly used by online services to stop users from sending age-inappropriate, bullying messages, or asking others to expose personal information. Previous work has focused on rules or classifiers to detect and filter offensive messages, but these are vulnerable to cleverly disguised plaintext and unseen expressions especially in an adversarial setting where the users can repeatedly try to bypass the filter. In this paper, we model the disguised messages as if they are produced by encrypting the original message using an invented cipher. We apply automatic decipherment techniques to decode the disguised malicious text, which can be then filtered using rules or classifiers. We provide experimental results on three different datasets and show that decipherment is an effective tool for this task.</p>
    </span>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2024 Nishant Kambhatla.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: April 03, 2024.
    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.0/umd/popper.min.js" integrity="sha256-OH05DFHUWzr725HmuHo3pnuvUUn+TJuj8/Qz9xytFEw=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/js/mdb.min.js"  integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js" integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>







</html>
